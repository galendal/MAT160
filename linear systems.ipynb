{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da270a44",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra\n",
    "Based on Turner et al. \n",
    "\n",
    "We're looking at \n",
    "$$\n",
    "A{\\bf x}={\\bf b}\n",
    "$$\n",
    "where $A$ is a $nxn$ matrix, and ${\\bf x}$  and ${\\bf b}$ are $n$-dimensional vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6f469",
   "metadata": {},
   "source": [
    "## Matrix operations in Python\n",
    "Matrix operations are fundamental in numerical computing, and Python, with the help of the NumPy library, provides efficient and intuitive tools for working with matrices. NumPy supports a wide range of matrix operations, including addition, multiplication, transposition, inversion, and decomposition. These capabilities make it a powerful choice for scientific computing, data analysis, and machine learning tasks. In this section, we will explore how to perform basic and advanced matrix operations using NumPy.\n",
    "\n",
    "- **Matrix addition:**  \n",
    "    `C = A + A`  &nbsp;# Element-wise addition\n",
    "\n",
    "- **Matrix multiplication:**  \n",
    "    `C = A @ A`  &nbsp;# Matrix product (dot product)\n",
    "\n",
    "- **Matrix vector multiplication:**  \n",
    "    `w = A @ v`  &nbsp;# Matrix vector product (dot product)\n",
    "    \n",
    "- **Vector dot product:**\n",
    "    `s = w @ v`  &nbsp;# Matrix vector product (dot product)\n",
    "\n",
    "- **Transpose:**  \n",
    "    `A_T = A.T`\n",
    "\n",
    "- **Inverse:**  \n",
    "    `A_inv = np.linalg.inv(A)`\n",
    "\n",
    "- **Determinant:**  \n",
    "    `det_A = np.linalg.det(A)`\n",
    "\n",
    "- **Extract diagonal:**  \n",
    "    `diag_A = np.diag(A)`\n",
    "\n",
    "- **Solve linear system $Ax = b$:**  \n",
    "    `x = np.linalg.solve(A, b)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afc80feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[ 6  8]\n",
      " [10 12]]\n",
      "\n",
      "Matrix Multiplication:\n",
      " [[19 22]\n",
      " [43 50]]\n",
      "\n",
      "Vector Addition: [4 6]\n",
      "Dot Product: 11\n",
      "Norm of v1: 2.23606797749979\n",
      "\n",
      "Matrix-Vector Multiplication (A @ v1): [ 5 11]\n",
      "Element-wise multiplication (A * v1):\n",
      " [[1 4]\n",
      " [3 8]]\n",
      "\n",
      "Transpose of A:\n",
      " [[1 3]\n",
      " [2 4]]\n",
      "\n",
      "Inverse of A:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n",
      "\n",
      "Determinant of A: -2.0000000000000004\n",
      "\n",
      "Diagonal of A: [1 4]\n",
      "\n",
      "Solution to Ax = b: [0.  0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Example vectors\n",
    "v1 = np.array([1, 2])\n",
    "v2 = np.array([3, 4])\n",
    "\n",
    "# Matrix addition\n",
    "C_add = A + B\n",
    "print(\"Matrix Addition:\\n\", C_add)\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "C_mul = A @ B\n",
    "print(\"\\nMatrix Multiplication:\\n\", C_mul)\n",
    "\n",
    "# Vector addition\n",
    "v_add = v1 + v2\n",
    "print(\"\\nVector Addition:\", v_add)\n",
    "\n",
    "# Dot product\n",
    "v_dot = np.dot(v1, v2)\n",
    "print(\"Dot Product:\", v_dot)\n",
    "\n",
    "# Norm of a vector\n",
    "v1_norm = np.linalg.norm(v1)\n",
    "print(\"Norm of v1:\", v1_norm)\n",
    "\n",
    "# Matrix vector multiplication\n",
    "Av1 = A @ v1\n",
    "print(\"\\nMatrix-Vector Multiplication (A @ v1):\", Av1)\n",
    "\n",
    "print(f\"Element-wise multiplication (A * v1):\\n {A * v1}\")\n",
    "\n",
    "# Transpose\n",
    "A_T = A.T\n",
    "print(\"\\nTranspose of A:\\n\", A_T)\n",
    "\n",
    "# Inverse\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(\"\\nInverse of A:\\n\", A_inv)\n",
    "\n",
    "# Determinant\n",
    "det_A = np.linalg.det(A)\n",
    "print(\"\\nDeterminant of A:\", det_A)\n",
    "\n",
    "# Extract diagonal\n",
    "diag_A = np.diag(A)\n",
    "print(\"\\nDiagonal of A:\", diag_A)\n",
    "\n",
    "# Solve linear system Ax = b\n",
    "b = np.array([1, 2])\n",
    "x = np.linalg.solve(A, b)\n",
    "print(\"\\nSolution to Ax = b:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4670ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(np.linalg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36980c",
   "metadata": {},
   "source": [
    "## Gauss elimination\n",
    "Treated in courses in linear algebra. Here is a demonstration. \n",
    "\n",
    "Might have to pivot rows to avoid dividing with zero and to prevent round off errors. \n",
    "\n",
    "To be aware of in numerical Gaussian elimination; division by zero, round off errors, dividing with small numbers, subtracting small numbers ...\n",
    "\n",
    "A short demonstration below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gauss_elimination(A, b, pivoting=False,verbatim=True): # Slightly different than the code in Turner et al. \n",
    "    n = len(b) # Number of equations\n",
    "    # Forward elimination\n",
    "    if verbatim: print(f\"Original A:\\n {A}\\n\") # Print original matrix if verbatim is True\n",
    "    for i in range(n): \n",
    "        # Partial pivoting\n",
    "        max_row = np.argmax(abs(A[i:, i])) + i # Find the index of the maximum element in the current column\n",
    "        if pivoting and i != max_row: # Swap rows if needed\n",
    "            A[[i, max_row]] = A[[max_row, i]] # Swap rows in A\n",
    "            b[[i, max_row]] = b[[max_row, i]] # Swap corresponding elements in b\n",
    "        # Eliminate entries below pivot\n",
    "        for j in range(i+1, n):\n",
    "            factor = A[j, i] / A[i, i]\n",
    "            A[j, i:] -= factor * A[i, i:]\n",
    "            b[j] -= factor * b[i]\n",
    "        if verbatim: print(f\"Matrix A after iteration {i+1}:\\n{A}\\n\")\n",
    "    # Back substitution\n",
    "    x = np.zeros(n) # Initialize solution vector\n",
    "    for i in range(n-1, -1, -1):\n",
    "        x[i] = (b[i] - np.dot(A[i, i+1:], x[i+1:])) / A[i, i] # Back substitution\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "141e74c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original A:\n",
      " [[1. 3. 5.]\n",
      " [3. 5. 5.]\n",
      " [5. 5. 5.]]\n",
      "\n",
      "Matrix A after iteration 1:\n",
      "[[  1.   3.   5.]\n",
      " [  0.  -4. -10.]\n",
      " [  0. -10. -20.]]\n",
      "\n",
      "Matrix A after iteration 2:\n",
      "[[  1.   3.   5.]\n",
      " [  0.  -4. -10.]\n",
      " [  0.   0.   5.]]\n",
      "\n",
      "Matrix A after iteration 3:\n",
      "[[  1.   3.   5.]\n",
      " [  0.  -4. -10.]\n",
      " [  0.   0.   5.]]\n",
      "\n",
      "matrix A: \n",
      " [[1. 3. 5.]\n",
      " [3. 5. 5.]\n",
      " [5. 5. 5.]]\n",
      "vector b:     [ 9. 13. 15.]\n",
      "\n",
      "System of equations:\n",
      "Eq1:   1.0*x1 +   3.0*x2 +   5.0*x3 =   9.0\n",
      "Eq2:   3.0*x1 +   5.0*x2 +   5.0*x3 =  13.0\n",
      "Eq3:   5.0*x1 +   5.0*x2 +   5.0*x3 =  15.0\n",
      "\n",
      "Solution:\n",
      "x1 = 1.0\n",
      "x2 = 1.0\n",
      "x3 = 1.0\n",
      "Solution: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "A = np.array([[1.0, 3.0, 5.0],\n",
    "              [3.0, 5.0, 5.0],\n",
    "              [5.0, 5.0, 5.0]])\n",
    "b = np.array([9.0, 13.0, 15.0])\n",
    "x = gauss_elimination(A.copy(), b.copy())\n",
    "print(\"matrix A: \\n\", A)\n",
    "print(\"vector b:    \", b)\n",
    "print(\"\\nSystem of equations:\")\n",
    "for i in range(A.shape[0]):\n",
    "    row = \" + \".join([f\"{A[i, j]:>5}*x{j+1}\" if A[i, j] >= 0 else f\"- {abs(A[i, j]):>4}*x{j+1}\" for j in range(A.shape[1])])\n",
    "    print(f\"Eq{i+1}: {row} = {b[i]:>5}\")\n",
    "\n",
    "print(\"\\nSolution:\")\n",
    "for i, val in enumerate(x, 1):\n",
    "    print(f\"x{i} = {val}\")\n",
    "print(\"Solution:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee30e5",
   "metadata": {},
   "source": [
    "**Task**\n",
    "Use the example code above to solve Example 3 on p. 87 in Turner \n",
    "$$\n",
    "\\left[\\begin{array}{rrr}\n",
    "7 & -7 & 1 \\\\\n",
    "-3 & 3 & 2 \\\\\n",
    "7 & 7 & -72 \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\\begin{array}{rrr}\n",
    "x \\\\\n",
    "y\\\\\n",
    "z\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\\begin{array}{rrr}\n",
    "1\\\\\n",
    "2\\\\\n",
    "7\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "with and without pivoting (change the pivoting argument in the Python cell below). What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18661ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "A = np.array([[7,-7,1],\n",
    "              [-3.0, 3.0, 2.0],\n",
    "              [7.0, 7.0, -72.0]])\n",
    "b = np.array([1.0, 2.0, 7.0])\n",
    "x = gauss_elimination(A.copy(), b.copy(),pivoting=False) # Change the pivoting option to see what happens\n",
    "print(\"matrix A: \\n\", A)\n",
    "print(\"vector b:    \", b)\n",
    "print(\"\\nSystem of equations:\")\n",
    "for i in range(A.shape[0]):\n",
    "    row = \" + \".join([f\"{A[i, j]:>5}*x{j+1}\" if A[i, j] >= 0 else f\"- {abs(A[i, j]):>4}*x{j+1}\" for j in range(A.shape[1])])\n",
    "    print(f\"Eq{i+1}: {row} = {b[i]:>5}\")\n",
    "\n",
    "print(\"\\nSolution:\")\n",
    "for i, val in enumerate(x, 1):\n",
    "    print(f\"x{i} = {val}\")\n",
    "print(\"Solution:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951622f",
   "metadata": {},
   "source": [
    "### Hilbert matrices\n",
    "\n",
    "Hilbert matrices are a special class of square matrices with elements given by \n",
    "$$H_{ij} = \\frac{1}{i + j - 1},\n",
    "$$\n",
    " where $i$ and $j$ are the row and column indices, respectively. These matrices are symmetric and positive definite, making them important in numerical analysis and linear algebra. However, Hilbert matrices are also notoriously ill-conditioned, meaning that small changes in input can lead to large errors in solutions when solving linear systems involving them. This property makes Hilbert matrices a classic example for testing the stability and accuracy of numerical algorithms.\n",
    "\n",
    "The $5 \\times 5$ Hilbert matrix:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} \\\\\n",
    "\\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} \\\\\n",
    "\\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} \\\\\n",
    "\\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} & \\frac{1}{8} \\\\\n",
    "\\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} & \\frac{1}{8} & \\frac{1}{9}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's do Example 6 on p. 92\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fa8396b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution with precision 4: [1. 1. 1. 1. 1. 1.] \n",
      "solution with precision 15: [0.999999999999228 1.000000000021937 0.999999999851792 1.00000000038537\n",
      " 0.999999999574584 1.00000000016768 ] \n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import hilbert\n",
    "import numpy as np\n",
    "H=hilbert(6)\n",
    "rsH = np.sum(H, axis=1)\n",
    "x = gauss_elimination(H, rsH, verbatim=False)\n",
    "prec=4\n",
    "np.set_printoptions(precision=prec)\n",
    "print(f\"solution with precision {prec}: {x} \")\n",
    "prec=15\n",
    "np.set_printoptions(precision=prec)\n",
    "print(f\"solution with precision {prec}: {x} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04c290",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Do this with a 10x10 Hilbert matrix. Do you get the same result as in the book?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185dd849",
   "metadata": {},
   "source": [
    "### Testing the Gauss elimination routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fbf6581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max error for each system size:\n",
      "n=4: 2.52e-14, mean=2.01e-15, #larger than 1.e-14: 5\n",
      "n=5: 1.14e-13, mean=6.26e-15, #larger than 1.e-14: 12\n",
      "n=6: 1.00e-12, mean=1.84e-14, #larger than 1.e-14: 17\n",
      "n=7: 5.96e-13, mean=3.13e-14, #larger than 1.e-14: 31\n",
      "n=8: 6.62e-13, mean=2.85e-14, #larger than 1.e-14: 36\n",
      "n=9: 4.57e-12, mean=1.21e-13, #larger than 1.e-14: 45\n",
      "n=10: 7.13e-13, mean=4.34e-14, #larger than 1.e-14: 57\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # For reproducibility\n",
    "E = np.zeros((100, 7)) # Error matrix\n",
    "for n in range(4, 11): # Iterate over different system sizes\n",
    "    for k in range(100):\n",
    "        A = np.random.uniform(-5, 5, size=(n, n)) # Random n x n matrix\n",
    "        b = np.sum(A, axis=1) # Right-hand side vector\n",
    "        x = gauss_elimination(A, b,verbatim=False) # Solve system\n",
    "        E[k, n-4] = np.max(np.abs(x - 1)) # Error calculation\n",
    "\n",
    "print(f\"Max error for each system size:\")\n",
    "for n in range(7):\n",
    "    print(f\"n={n+4}: {np.max(E[:, n]):.2e}, mean={np.mean(E[:, n]):.2e}, #larger than 1.e-14: {np.sum(E[:, n] > 1.e-14)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60ecb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c021dcc",
   "metadata": {},
   "source": [
    "## Tri-diagonal matrices\n",
    "Tri-diagonal matrices are a special type of sparse matrix where nonzero elements appear only on the main diagonal, the diagonal above it, and the diagonal below it. In other words, all other entries are zero. These matrices commonly arise in the discretization of differential equations, especially in finite difference methods for solving boundary value problems. Their structure allows for highly efficient algorithms, such as the Thomas algorithm, to solve linear systems involving tri-diagonal matrices with significantly reduced computational effort compared to general dense matrices.\n",
    "The one-dimensional heat equation describes how heat diffuses through a rod over time and is given by:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2} + f(x,t)\n",
    "$$\n",
    "where $u(x, t)$ is the temperature at position $x$ and time $t$, and $\\alpha$ is the thermal diffusivity.\n",
    "\n",
    "### Implicit Scheme (Backward Euler)\n",
    "\n",
    "For numerical solutions, we discretize both space and time. Let $u_j^n$ denote the temperature at position $x_j$ and time $t_n$. The implicit (backward Euler) scheme for the heat equation is:\n",
    "$$\n",
    "\\frac{u_j^{n+1} - u_j^n}{\\Delta t} = \\alpha \\frac{u_{j+1}^{n+1} - 2u_j^{n+1} + u_{j-1}^{n+1}}{(\\Delta x)^2} + f_j^{n+1}\n",
    "$$\n",
    "This leads to a linear system for $u^{n+1}$ at each time step, which can be written as a tri-diagonal matrix equation:\n",
    "$$\n",
    "A \\mathbf{u}^{n+1} = \\mathbf{u}^n + \\mathbf{f}^n\n",
    "$$\n",
    "where $A$ is a tri-diagonal matrix with main diagonal $1 + 2r$, and off-diagonals $-r$, with $r = \\alpha \\Delta t / (\\Delta x)^2$.\n",
    "\n",
    "The tri-diagonal structure of $A$ allows efficient solution using specialized algorithms.\n",
    "\n",
    "**Applications:**  \n",
    "Implicit methods are unconditionally stable and allow for larger time steps compared to explicit methods, making them suitable for stiff problems and long-time simulations. The tri-diagonal matrix structure arises naturally from the finite difference discretization of the second derivative in space.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2dc6a",
   "metadata": {},
   "source": [
    "# LU factorization\n",
    "\n",
    "LU factorization is a method of decomposing a square matrix $A$ into the product of a lower triangular matrix $L$ and an upper triangular matrix $U$, such that $A = LU$. This technique simplifies the process of solving systems of linear equations, computing determinants, and inverting matrices. LU factorization is widely used in numerical linear algebra due to its efficiency and stability, especially for large systems.\n",
    "\n",
    "LU factorization allows us to efficiently solve multiple systems of linear equations with the same coefficient matrix $A$ but different right-hand sides $b$. Once $A$ is decomposed into $L$ and $U$, we can solve $Ax = b$ by first solving $Ly = b$ (forward substitution) and then $Ux = y$ (back substitution), which is computationally cheaper than performing Gaussian elimination for each new $b$. This is especially useful in applications such as simulations, optimization, and engineering problems where the same system structure is reused with varying inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d497e8",
   "metadata": {},
   "source": [
    "### LU factorization and Gaussian elimination \n",
    "LU factorization and Gaussian elimination are fundamentally connected methods for solving linear systems.\n",
    "\n",
    "- **Gaussian elimination** systematically applies row operations to reduce a matrix $A$ to upper triangular form $U$. Each elimination step can be represented by multiplying $A$ by an elementary lower triangular matrix $M_i$:\n",
    "    $$\n",
    "    M_{n-1} \\cdots M_2 M_1 A = U\n",
    "    $$\n",
    "    where each $M_i$ eliminates entries below the diagonal in column $i$.\n",
    "\n",
    "- **LU factorization** rewrites $A$ as the product of a lower triangular matrix $L$ and an upper triangular matrix $U$:\n",
    "    $$\n",
    "    A = LU\n",
    "    $$\n",
    "    Here, $L$ is constructed from the inverses of the elimination matrices:\n",
    "    $$\n",
    "    L = M_1^{-1} M_2^{-1} \\cdots M_{n-1}^{-1}\n",
    "    $$\n",
    "    and $U$ is the final upper triangular matrix from elimination.\n",
    "\n",
    "**Summary with matrix notation:**  \n",
    "Performing Gaussian elimination is equivalent to finding matrices $L$ and $U$ such that $A = LU$. The process can be written as:\n",
    "$$\n",
    "A = L U \\implies L^{-1}A = U\n",
    "$$\n",
    "where $L$ contains the multipliers used during elimination (below the diagonal), and $U$ is the result of the elimination (upper triangular). This decomposition allows us to solve $A\\mathbf{x} = \\mathbf{b}$ efficiently by first solving $L\\mathbf{y} = \\mathbf{b}$ wrt $\\mathbf{y}$ (forward substitution), then $U\\mathbf{x} = \\mathbf{y}$ wrt $\\mathbf{x}$ (back substitution).\n",
    "\n",
    "**In case of pivoting:**  \n",
    "$$\n",
    "PA=LU \\Rightarrow PA\\mathbf{x} = LU \\mathbf{x} = Pb\n",
    "$$\n",
    "we need to pivot the right hand side as well:\n",
    "$$\n",
    " L U \\mathbf{x}= P\\mathbf{b} \n",
    "$$\n",
    "\n",
    "**Note** scipy.linalg.lu returns $A=PLU$, but we know that $P^T=P^{-1}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d282d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "\n",
      "Permutation matrix P:\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "Lower triangular matrix L:\n",
      "[[1.                0.                0.               ]\n",
      " [0.142857142857143 1.                0.               ]\n",
      " [0.571428571428571 0.5               1.               ]]\n",
      "\n",
      "Upper triangular matrix U:\n",
      "[[ 7.                 8.                 9.               ]\n",
      " [ 0.                 0.857142857142857  1.714285714285714]\n",
      " [ 0.                 0.                -0.               ]]\n",
      "\n",
      "Check: A == P @ L @ U ?\n",
      "True\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "\n",
      "P @ L @ U:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import lu\n",
    "A=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "# Perform LU factorization of matrix A\n",
    "P, L, U = lu(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nPermutation matrix P:\")\n",
    "print(P)\n",
    "print(\"\\nLower triangular matrix L:\")\n",
    "print(L)\n",
    "print(\"\\nUpper triangular matrix U:\")\n",
    "print(U)\n",
    "\n",
    "# Verify the factorization: P @ A = L @ U\n",
    "print(\"\\nCheck: A == P @ L @ U ?\")\n",
    "print(np.allclose( A, P @L @ U))\n",
    "print( A== P @L @ U)\n",
    "print(\"\\nP @ L @ U:\")\n",
    "print(P @ L @ U)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c8182",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "Do LU factorization the $A$ matrix in Example 3 on p. 87 in Turner using _scipy.linalg.lu_, then solve the system with forward and backward substitution. Solve the system using Gaussian elimination. Time the LU factorization, the forward and backward substitution and the Gaussian elimination solver. The problem is.  \n",
    "$$\n",
    "\\left[\\begin{array}{rrr}\n",
    "7 & -7 & 1 \\\\\n",
    "-3 & 3 & 2 \\\\\n",
    "7 & 7 & -72 \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\\begin{array}{rrr}\n",
    "x \\\\\n",
    "y\\\\\n",
    "z\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\\begin{array}{rrr}\n",
    "1\\\\\n",
    "2\\\\\n",
    "7\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a0db9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code goes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f271c88",
   "metadata": {},
   "source": [
    "**Task:** Timing the routines\n",
    "\n",
    "The following algorithm produces a diagonal dominant random matrix of dimension 5x5 and a right had side $b$ that is equal to the sum of the corresponding rows in $A$. \n",
    "\n",
    "- n=5 \n",
    "- np.random.seed(42) # For reproducibility\n",
    "- A = 10 * np.eye(n) # Diagonal matrix\n",
    "- A += np.random.rand(n, n)\n",
    "- b = np.sum(A, axis=1)\n",
    "\n",
    "Make a loop with increasing dimensions, $n$. For each $n$, solve the resulting problem using \n",
    "    a) Gaussian elimination, \n",
    "    b) use scipy to calculate the LU factorization of A, \n",
    "    c) and thereafter use the LU factorization to solve the problem using forward and backward substitution. \n",
    "    \n",
    "Time each of these and see how the time changes with increasing $n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577229f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n=5 \n",
    "np.random.seed(42) # For reproducibility\n",
    "A = 10 * np.eye(n) # Diagonal matrix\n",
    "A += np.random.rand(n, n)\n",
    "b = np.sum(A, axis=1)\n",
    "\n",
    "# You do the rest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0135e",
   "metadata": {},
   "source": [
    "# Iterative methods. \n",
    "Iterative methods are algorithms used to solve systems of linear equations by generating a sequence of approximations that converge to the exact solution. Unlike direct methods (such as Gaussian elimination), which attempt to solve the system in a finite number of steps, iterative methods start with an initial guess and improve it through repeated updates. \n",
    "\n",
    "We need iterative methods because they are often more efficient and practical for large, sparse, or structured systems where direct methods become computationally expensive or require excessive memory. Iterative techniques, such as the Jacobi and Gauss-Seidel methods, are especially useful in scientific computing, engineering simulations, and when dealing with matrices that arise from discretizing partial differential equations. They can exploit matrix sparsity, are easier to parallelize, and can provide approximate solutions quickly when high precision is not required.\n",
    "\n",
    "### Jacobi Method Equations\n",
    "\n",
    "Given a linear system $A\\mathbf{x} = \\mathbf{b}$, the Jacobi method updates each component of $\\mathbf{x}$ iteratively as follows:\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} a_{ij} x_j^{(k)} \\right), \\quad i = 1, 2, \\ldots, n\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i^{(k+1)}$ is the new value of the $i$-th variable at iteration $k+1$,\n",
    "- $a_{ii}$ is the diagonal element of $A$,\n",
    "- $b_i$ is the $i$-th entry of $\\mathbf{b}$,\n",
    "- $a_{ij}$ are the off-diagonal elements of $A$,\n",
    "- $x_j^{(k)}$ are the values from the previous iteration.\n",
    "\n",
    "All updates for $x_i^{(k+1)}$ use only the values from the previous iteration $k$. This makes the Jacobi method easy to parallelize.\n",
    "\n",
    "In matrix-vector form, the Jacobi iteration can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = D^{-1} \\left( \\mathbf{b} - (L + U)\\mathbf{x}^{(k)} \\right)\n",
    "$$\n",
    "\n",
    "where $A = D + L + U$ with:\n",
    "- $D$ the diagonal part of $A$,\n",
    "- $L$ the strictly lower triangular part,\n",
    "- $U$ the strictly upper triangular part.\n",
    "\n",
    "This shows that each new iterate $\\mathbf{x}^{(k+1)}$ is computed using only the previous iterate $\\mathbf{x}^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi (A , b , Nits ) :  # Jacobi method\n",
    "    \"\"\" Function for computing  Nits  iterations \n",
    "    of the Jacobi method for Ax=b \n",
    "    where A  must be square matrix.  \"\"\"\n",
    "    D = np . diag ( A ) # Extract diagonal\n",
    "    n = A . shape [0]\n",
    "    A_D = A - np . diag ( D ) # This is L+U\n",
    "    x = np . zeros ( n ) # Initial guess\n",
    "    s = np . zeros ( ( n , Nits ) ) # Store all iterations\n",
    "    for k in range ( Nits ) :\n",
    "        x = ( b - A_D . dot ( x ) ) / D\n",
    "        s [: , k] = x\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "62b41301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "[[10.374540118847362  0.950714306409916  0.731993941811405]\n",
      " [ 0.598658484197037 10.156018640442436  0.155994520336203]\n",
      " [ 0.058083612168199  0.866176145774935 10.60111501174321 ]]\n",
      "S:\n",
      "[[1.16219593629643  1.074305988522719 1.087185145800164]\n",
      " [0.987039159004468 0.989100022136149 0.993040071678878]\n",
      " [1.001489934983541 1.000870895221589 1.000961607648404]\n",
      " [0.999852343959151 0.999897403915889 0.999920679148258]\n",
      " [1.000014998442928 1.000009922112531 1.000009191738493]\n",
      " [0.999998442206494 0.999998974715751 0.999999107125909]]\n",
      "S.shape:\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "dim=3\n",
    "np.random.seed(42) # For reproducibility\n",
    "A = 10 * np.eye(dim) # Diagonal matrix\n",
    "A += np.random.rand(dim, dim)\n",
    "b = np.sum(A, axis=1)\n",
    "print(f\"A:\\n{A}\")\n",
    "S = jacobi(A, b, 6)\n",
    "print(f\"S:\\n{S.T}\")\n",
    "print(f\"S.shape:\\n{S.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "05d1c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: error = 2.246560997625002\n",
      "Iteration 2: error = 0.2093305160845364\n",
      "Iteration 3: error = 0.022542288300438085\n",
      "Iteration 4: error = 0.0022436253150079624\n",
      "Iteration 5: error = 0.00023085368550226017\n",
      "Iteration 6: error = 2.3610369329478668e-05\n"
     ]
    }
   ],
   "source": [
    "errors = np.linalg.norm(A @ S - b[:, None], axis=0)\n",
    "for i, err in enumerate(errors):\n",
    "    print(f\"Iteration {i+1}: error = {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6bd2a8",
   "metadata": {},
   "source": [
    "### Gauss-Seidel Method Equations\n",
    "\n",
    "The Gauss-Seidel method is an iterative technique for solving a linear system $A\\mathbf{x} = \\mathbf{b}$. It improves upon the Jacobi method by using the most recently updated values as soon as they are available.\n",
    "\n",
    "The update rule for each component $x_i$ at iteration $k+1$ is:\n",
    "\n",
    "$$\n",
    "x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(k)} \\right), \\quad i = 1, 2, \\ldots, n\n",
    "$$\n",
    "\n",
    "- $x_i^{(k+1)}$ uses the latest values $x_j^{(k+1)}$ for $j < i$ (already updated in the current iteration).\n",
    "- $x_j^{(k)}$ for $j > i$ are from the previous iteration.\n",
    "\n",
    "**Key points:**\n",
    "- The method converges faster than Jacobi for many problems, especially when $A$ is diagonally dominant or symmetric positive definite.\n",
    "- Each new value is immediately used in subsequent calculations within the same iteration.\n",
    "\n",
    "In **matrix-vector form**, the Gauss-Seidel iteration can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = -(D+L)^{-1} U\\,\\mathbf{x}^{(k)} + (D+L)^{-1}\\mathbf{b}=G_{GS}\\mathbf{x}^{(k)}+\\mathbf{c}\n",
    "$$\n",
    "where $A = D + L + U$ with:\n",
    "- $D$ the diagonal of $A$,\n",
    "- $L$ the strictly lower triangular part,\n",
    "- $U$ the strictly upper triangular part.\n",
    "\n",
    "This equation shows that each new iterate $\\mathbf{x}^{(k+1)}$ uses the latest available values for the lower part and previous values for the upper part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0a3132ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss-Seidel iterations (columns are successive approximations):\n",
      "[[1.16219593629643  0.999564718822089 0.999995831218039 0.999999975411345\n",
      "  0.999999999867319 0.9999999999993  ]\n",
      " [1.005798979802532 1.000046585678367 1.000000267566598 1.000000001434371\n",
      "  1.000000000007552 1.00000000000004 ]\n",
      " [0.998637514655969 0.999998578574028 1.000000000978964 1.000000000017525\n",
      "  1.00000000000011  1.000000000000001]]\n"
     ]
    }
   ],
   "source": [
    "def gauss_seidel(A, b, Nits):\n",
    "    \"\"\"Performs Nits iterations of the Gauss-Seidel method for Ax=b.\"\"\"\n",
    "    n = A.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    s = np.zeros((n, Nits))\n",
    "    for k in range(Nits):\n",
    "        for i in range(n):\n",
    "            sum1 = np.dot(A[i, :i], x[:i])\n",
    "            sum2 = np.dot(A[i, i+1:], x[i+1:])\n",
    "            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n",
    "        s[:, k] = x\n",
    "    return s\n",
    "\n",
    "# Demonstration with the same A and b as before\n",
    "S_gs = gauss_seidel(A, b, 6)\n",
    "print(\"Gauss-Seidel iterations (columns are successive approximations):\")\n",
    "print(S_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c076acd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: error = 1.6944005300073743\n",
      "Iteration 2: error = 0.004477629499036153\n",
      "Iteration 3: error = 4.2994672271196375e-05\n",
      "Iteration 4: error = 2.537195270839496e-07\n",
      "Iteration 5: error = 1.3692469708357727e-09\n",
      "Iteration 6: error = 7.222688971441994e-12\n"
     ]
    }
   ],
   "source": [
    "errors_gs = np.linalg.norm(A @ S_gs - b[:, None], axis=0)\n",
    "for i, err in enumerate(errors_gs):\n",
    "    print(f\"Iteration {i+1}: error = {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a6783",
   "metadata": {},
   "source": [
    "## Eigenvalues\n",
    "Eigenvalues are fundamental quantities associated with a square matrix that provide deep insight into its structure and behavior. For a given $n \\times n$ matrix $A$, an eigenvalue $\\lambda$ and a corresponding nonzero vector $\\mathbf{v}$ (called an eigenvector) satisfy the equation\n",
    "$$\n",
    "A\\mathbf{v} = \\lambda \\mathbf{v}.\n",
    "$$\n",
    "This means that applying the matrix $A$ to the vector $\\mathbf{v}$ simply scales $\\mathbf{v}$ by the factor $\\lambda$, without changing its direction. Eigenvalues play a crucial role in many areas of mathematics, physics, and engineering, including stability analysis, vibrations, quantum mechanics, and principal component analysis. They are also central to understanding the long-term behavior of dynamical systems and are used in matrix diagonalization and decomposition techniques. The set of all eigenvalues of a matrix is called its spectrum. Previously we talked about the _spectral radius_, the largest absolute value\n",
    "of the eigenvalues of a matrix. The **spectral radius** $\\rho(A)$ of a matrix $A$ is defined as\n",
    "$$\n",
    "\\rho(A) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}\n",
    "$$\n",
    "That is, it is the largest absolute value among all the eigenvalues of $A$. The spectral radius is important in determining the stability of iterative methods and dynamical systems. For example, in iterative solvers, convergence is guaranteed if the spectral radius of the iteration matrix is less than 1.\n",
    "\n",
    "### Condition Numbers and Ill-Conditioned Matrices\n",
    "\n",
    "The **condition number** of a matrix is a measure of how sensitive the solution of a linear system is to small changes or errors in the input data. For a square matrix $A$, the condition number with respect to inversion (in the 2-norm) is defined as:\n",
    "\n",
    "For symmetric positive definite matrices, the condition number can also be expressed as the ratio of the largest to the smallest eigenvalue:\n",
    "$$\n",
    "\\kappa(A) = \\frac{|\\lambda_{\\max}|}{|\\lambda_{\\min}|}\n",
    "$$\n",
    "where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest (in absolute value) eigenvalues of $A$. This form highlights the connection between the condition number and the spectrum of the matrix. If the smallest eigenvalue is close to zero, the matrix is nearly singular and highly ill-conditioned.\n",
    "\n",
    "- If $\\kappa(A)$ is close to 1, the matrix is **well-conditioned**: small changes in the input or right-hand side produce small changes in the solution.\n",
    "- If $\\kappa(A)$ is large (much greater than 1), the matrix is **ill-conditioned**: even tiny errors in the data can cause large errors in the solution.\n",
    "\n",
    "**Ill-conditioned matrices** are problematic in numerical computations because they amplify rounding errors and make solutions unreliable. Hilbert matrices are classic examples of ill-conditioned matrices, as their condition number grows rapidly with size.\n",
    "\n",
    "The condition number also provides insight into the stability of algorithms: a high condition number indicates that the problem is inherently difficult to solve accurately, regardless of the algorithm used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416f482",
   "metadata": {},
   "source": [
    "### The power method to find the larges eigenvalue\n",
    "The **power method** is an iterative algorithm used to estimate the largest (in absolute value) eigenvalue and its corresponding eigenvector of a square matrix $A$. The method is especially useful for large matrices where computing all eigenvalues is computationally expensive.\n",
    "\n",
    "**How it works:**\n",
    "1. Start with an initial nonzero vector $\\mathbf{x}_0$.\n",
    "2. At each iteration, compute $\\mathbf{x}_{k+1} = A\\mathbf{x}_k$.\n",
    "3. Normalize $\\mathbf{x}_{k+1}$ to prevent overflow or underflow.\n",
    "4. The sequence of vectors $\\mathbf{x}_k$ converges to the eigenvector associated with the largest eigenvalue (in magnitude).\n",
    "5. The Rayleigh quotient $\\lambda_k = \\frac{\\mathbf{x}_k^T A \\mathbf{x}_k}{\\mathbf{x}_k^T \\mathbf{x}_k}$ gives an estimate of the dominant eigenvalue.\n",
    "\n",
    "**Algorithm steps:**\n",
    "1. Choose an initial vector $\\mathbf{x}_0$ (not orthogonal to the dominant eigenvector).\n",
    "2. For $k = 1, 2, \\ldots, N$ (until convergence):\n",
    "    - $\\mathbf{y}_k = A \\mathbf{x}_{k-1}$\n",
    "    - $\\mathbf{x}_k = \\mathbf{y}_k / \\|\\mathbf{y}_k\\|$\n",
    "    - $\\lambda_k = \\mathbf{x}_k^T A \\mathbf{x}_k$\n",
    "3. $\\lambda_k$ approximates the largest eigenvalue, and $\\mathbf{x}_k$ the corresponding eigenvector.\n",
    "\n",
    "**Key points:**\n",
    "- The method converges if $A$ has a unique eigenvalue of largest magnitude.\n",
    "- The rate of convergence depends on the ratio of the largest to the second largest eigenvalue.\n",
    "- The power method is simple and efficient for finding the dominant eigenvalue of large, sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e40f1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(A, num_iters=1000, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the largest eigenvalue and corresponding eigenvector of a square matrix A using the power method.\n",
    "    Args:\n",
    "        A: numpy.ndarray, square matrix\n",
    "        num_iters: int, maximum number of iterations\n",
    "        tol: float, convergence tolerance\n",
    "    Returns:\n",
    "        eigenvalue: float, largest eigenvalue\n",
    "        eigenvector: numpy.ndarray, corresponding eigenvector (normalized)\n",
    "    \"\"\"\n",
    "    A = np.array(A, dtype=float)\n",
    "    n = A.shape[0]\n",
    "    x = np.random.rand(n)\n",
    "    x /= np.linalg.norm(x)\n",
    "    eigenvalue_old = 0.0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        x_new = A @ x\n",
    "        x_new_norm = np.linalg.norm(x_new)\n",
    "        if x_new_norm == 0:\n",
    "            raise ValueError(\"Encountered zero vector during iteration.\")\n",
    "        x = x_new / x_new_norm\n",
    "        eigenvalue = x @ (A @ x)\n",
    "        if np.abs(eigenvalue - eigenvalue_old) < tol:\n",
    "            break\n",
    "        eigenvalue_old = eigenvalue\n",
    "\n",
    "    return eigenvalue, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74bf33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
